{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz7J_KzfFX4F",
        "outputId": "06b9da1a-02bf-4f65-9f73-fe66c0cc32a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "from google.colab import drive\n",
        "import os\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include the path of datas"
      ],
      "metadata": {
        "id": "Jvt_UIPx1sTL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs0Ec-Deo01O",
        "outputId": "7c4713f4-bb19-4d23-83d4-9904a3e7fa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 734 images belonging to 41 classes.\n",
            "Found 167 images belonging to 41 classes.\n",
            "Epoch 1/10\n",
            "23/23 [==============================] - 188s 8s/step - loss: 3.7347 - accuracy: 0.0232 - val_loss: 3.7135 - val_accuracy: 0.0240\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 38s 2s/step - loss: 3.7150 - accuracy: 0.0177 - val_loss: 3.7132 - val_accuracy: 0.0299\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 35s 2s/step - loss: 3.7111 - accuracy: 0.0368 - val_loss: 3.7119 - val_accuracy: 0.0240\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 34s 1s/step - loss: 3.6915 - accuracy: 0.0422 - val_loss: 3.6545 - val_accuracy: 0.0359\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 39s 2s/step - loss: 3.4762 - accuracy: 0.0804 - val_loss: 3.3202 - val_accuracy: 0.1078\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 37s 2s/step - loss: 3.1324 - accuracy: 0.1649 - val_loss: 3.0748 - val_accuracy: 0.1976\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 35s 2s/step - loss: 2.8256 - accuracy: 0.2330 - val_loss: 2.9756 - val_accuracy: 0.2216\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 40s 2s/step - loss: 2.5659 - accuracy: 0.3106 - val_loss: 3.2995 - val_accuracy: 0.2275\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 35s 2s/step - loss: 2.3777 - accuracy: 0.3365 - val_loss: 2.8764 - val_accuracy: 0.2455\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 37s 2s/step - loss: 2.1035 - accuracy: 0.4128 - val_loss: 2.8959 - val_accuracy: 0.2515\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 126, 126, 50)      500       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 50)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 65)        29315     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 30, 30, 65)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 85)        49810     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 85)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16660)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               2132608   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 41)                5289      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,217,522\n",
            "Trainable params: 2,217,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_path = '/content/drive/MyDrive/train/train'\n",
        "test_path = '/content/drive/MyDrive/test/test'\n",
        "\n",
        "# Set up the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/train/train', target_size=(128, 128), batch_size=32, color_mode='grayscale', class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/test/test', target_size=(128, 128), batch_size=32, color_mode='grayscale', class_mode='categorical')\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model =keras.Sequential([\n",
        "  layers.Conv2D(50, (3, 3), activation='relu', input_shape=(128,128,1)),\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  layers.Conv2D(65, (3, 3), activation='relu'),\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  layers.Conv2D(85, (3, 3), activation='relu'),\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, steps_per_epoch=len(train_generator), batch_size=32,epochs=10,verbose=1, validation_data=test_generator, validation_steps=len(test_generator))\n",
        "\n",
        "# Evaluate the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qecdqdtdL9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa676c2-aafb-41c5-cf68-f9cc727c40e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 734 images belonging to 41 classes.\n",
            "Found 167 images belonging to 41 classes.\n",
            "Epoch 1/30\n",
            "23/23 [==============================] - 36s 2s/step - loss: 3.7222 - accuracy: 0.0163 - val_loss: 3.7142 - val_accuracy: 0.0240\n",
            "Epoch 2/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 3.7138 - accuracy: 0.0327 - val_loss: 3.7116 - val_accuracy: 0.0180\n",
            "Epoch 3/30\n",
            "23/23 [==============================] - 38s 2s/step - loss: 3.6941 - accuracy: 0.0545 - val_loss: 3.6615 - val_accuracy: 0.0419\n",
            "Epoch 4/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 3.5061 - accuracy: 0.0940 - val_loss: 3.3660 - val_accuracy: 0.0958\n",
            "Epoch 5/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 3.1783 - accuracy: 0.1526 - val_loss: 3.2947 - val_accuracy: 0.1377\n",
            "Epoch 6/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 2.8003 - accuracy: 0.2425 - val_loss: 3.0916 - val_accuracy: 0.1677\n",
            "Epoch 7/30\n",
            "23/23 [==============================] - 36s 2s/step - loss: 2.5817 - accuracy: 0.2752 - val_loss: 2.9682 - val_accuracy: 0.2216\n",
            "Epoch 8/30\n",
            "23/23 [==============================] - 38s 2s/step - loss: 2.3369 - accuracy: 0.3256 - val_loss: 2.7399 - val_accuracy: 0.2575\n",
            "Epoch 9/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 2.1398 - accuracy: 0.4060 - val_loss: 2.5959 - val_accuracy: 0.2874\n",
            "Epoch 10/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 1.9214 - accuracy: 0.4428 - val_loss: 2.9580 - val_accuracy: 0.3234\n",
            "Epoch 11/30\n",
            "23/23 [==============================] - 39s 2s/step - loss: 1.8163 - accuracy: 0.4823 - val_loss: 2.6529 - val_accuracy: 0.3174\n",
            "Epoch 12/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 1.6836 - accuracy: 0.5341 - val_loss: 2.6757 - val_accuracy: 0.3114\n",
            "Epoch 13/30\n",
            "23/23 [==============================] - 34s 2s/step - loss: 1.4509 - accuracy: 0.5708 - val_loss: 2.5123 - val_accuracy: 0.3653\n",
            "Epoch 14/30\n",
            "23/23 [==============================] - 36s 2s/step - loss: 1.3150 - accuracy: 0.6172 - val_loss: 2.4918 - val_accuracy: 0.3713\n",
            "Epoch 15/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 1.2416 - accuracy: 0.6362 - val_loss: 2.3967 - val_accuracy: 0.3653\n",
            "Epoch 16/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 1.1280 - accuracy: 0.6689 - val_loss: 2.7055 - val_accuracy: 0.3892\n",
            "Epoch 17/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 1.0554 - accuracy: 0.6962 - val_loss: 2.4330 - val_accuracy: 0.3892\n",
            "Epoch 18/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.9146 - accuracy: 0.7248 - val_loss: 2.2786 - val_accuracy: 0.4431\n",
            "Epoch 19/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.7491 - accuracy: 0.7493 - val_loss: 2.7320 - val_accuracy: 0.4371\n",
            "Epoch 20/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.6770 - accuracy: 0.8011 - val_loss: 2.3761 - val_accuracy: 0.4611\n",
            "Epoch 21/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.6364 - accuracy: 0.8134 - val_loss: 2.5295 - val_accuracy: 0.4491\n",
            "Epoch 22/30\n",
            "23/23 [==============================] - 38s 2s/step - loss: 0.5795 - accuracy: 0.8229 - val_loss: 2.3049 - val_accuracy: 0.4731\n",
            "Epoch 23/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.5499 - accuracy: 0.8256 - val_loss: 2.3078 - val_accuracy: 0.4910\n",
            "Epoch 24/30\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.4502 - accuracy: 0.8692 - val_loss: 2.6953 - val_accuracy: 0.4611\n",
            "Epoch 25/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.4400 - accuracy: 0.8583 - val_loss: 2.5156 - val_accuracy: 0.5150\n",
            "Epoch 26/30\n",
            "23/23 [==============================] - 34s 1s/step - loss: 0.3935 - accuracy: 0.8760 - val_loss: 2.7294 - val_accuracy: 0.4731\n",
            "Epoch 27/30\n",
            "23/23 [==============================] - 34s 1s/step - loss: 0.2980 - accuracy: 0.9046 - val_loss: 2.7973 - val_accuracy: 0.4970\n",
            "Epoch 28/30\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3143 - accuracy: 0.9033 - val_loss: 2.4999 - val_accuracy: 0.5210\n",
            "Epoch 29/30\n",
            "23/23 [==============================] - 34s 1s/step - loss: 0.3027 - accuracy: 0.9060 - val_loss: 2.5148 - val_accuracy: 0.5269\n",
            "Epoch 30/30\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.3068 - accuracy: 0.9087 - val_loss: 2.4587 - val_accuracy: 0.5569\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_path = '/content/drive/MyDrive/train/train'\n",
        "test_path = '/content/drive/MyDrive/test/test'\n",
        "\n",
        "# Set up the data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/train/train', target_size=(128, 128), batch_size=32, color_mode='grayscale', class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/test/test', target_size=(128, 128), batch_size=32, color_mode='grayscale', class_mode='categorical')\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(50, (3, 3), activation='relu', input_shape=(128,128,1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(65, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(85, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, steps_per_epoch=len(train_generator), batch_size=32,epochs=30,verbose=1, validation_data=test_generator, validation_steps=len(test_generator))\n",
        "\n",
        "# Evaluate the model\n",
        "# Evaluate the model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}